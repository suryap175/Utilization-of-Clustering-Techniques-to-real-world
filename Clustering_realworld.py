# -*- coding: utf-8 -*-
"""G61_FDA_Project_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1shOSNC5wWY4UuH5AMnrQtKnh30ycEHuw

## **FDA Project- 1**
# **Submission: 11/21/2022**
# **Surya Pratap Singh**

---



---

#**Task-1**

# **For Data 1**
"""

import pandas as pd #importing pandas
import numpy as np #importing numpy
from google.colab import files #importing files from local system
uploaded=files.upload() #shows the file uploaded from the local system.
import seaborn as sns  #importing seaborn for graphical purposes.
import matplotlib.pyplot as plt #importing matplotlib

data1=pd.read_csv('Data1.csv') #reading data1.csv from local system and assigning it to data1
data_refined1 = pd.DataFrame(data1, columns = ['X1', 'X2', 'X3', 'Class']) #reading columns 1,2,3 and 4 from the dataset and assigning it to a new variable data_refined1
data_refined1 = np.round(data_refined1, decimals = 3) #organizing the data by making all the values of same type by making all the values rounded to  3 decimal places.
data_refined1 #displaying data_refined1

Considered_Values= data_refined1.iloc[:,0:3].values #reading the values of columns and not considering the class and overall assigning it to Considered_Values. here, iloc is used for accessing the values based on selected columns.
Considered_Values #displaying Considered_Values

data_refined1.describe() #describing the refined data to understand highest, lowest and other related values in the columns. By this, we get a better understanding of how the data values in the dataset are separated and represented.

from sklearn.cluster import KMeans #importing kmeans
Understanding_kmeans = KMeans(n_clusters = 7, init = 'k-means++') #deciding number of clusters based on the classes of the data, and incrementing k++ and assigning the entire unit to new variable Understanding_kmeans
Predicting_kmeans = Understanding_kmeans.fit_predict(Considered_Values) #In this, we are predicting/ working with the kmeans algorithm using Understanding_kmeans.fit_predict and under this, Considered_Values is passed holds the column values of the main data.


fig = plt.figure(figsize = (10,10)) #deciding the size of the figure and assigning it to a new variable fig.
project = fig.add_subplot(111, projection='3d') #project variable holds the parameters for creating a 3D plot.


plt.scatter(Considered_Values[Predicting_kmeans == 0, 0], Considered_Values[Predicting_kmeans == 0, 1], s = 70, c = 'red', label = 'Cluster 1') #plotting the first cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 1, 0], Considered_Values[Predicting_kmeans == 1, 1], s = 70, c = 'blue', label = 'Cluster 2') #plotting the second cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 2, 0], Considered_Values[Predicting_kmeans == 2, 1], s = 70, c = 'green', label = 'Cluster 3') #plotting the third cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 3, 0], Considered_Values[Predicting_kmeans == 3, 1], s = 70, c = 'cyan', label = 'Cluster 4') #plotting the forth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 4, 0], Considered_Values[Predicting_kmeans == 4, 1], s = 70, c = 'magenta', label = 'Cluster 5') #plotting the fifth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 5, 0], Considered_Values[Predicting_kmeans == 5, 1], s = 70, c = 'orange', label = 'Cluster 6') #plotting the sixth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 6, 0], Considered_Values[Predicting_kmeans == 6, 1], s = 70, c = 'brown', label = 'Cluster 7') #plotting the seventh cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.

plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('K-Means Clustering General 3D ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(n): #defining a new function add that takes n
  return n+1 #returning n+1
Predicting_kmeans=np.array(list(map(add,Predicting_kmeans))) #mapping the values of the allocated clustering class.

# The main reason for doing the above code is because the classes given in the dataset are befinning from 1 but by default the classes that are generated for our training kmeans algorithm
# is beginning from 0 and because of this the comparision cannot be achieved for the actual classed and the allocated classes.
#Thus, to have similiar understanding of the actual classed and allocated classed, either the values of the class of actual classes should be decreased so that it begins from one
#or else the value of the allocated classes should be incremented by 1 (beginning form 1) so that there is a clear understanding.
#Thus, by above function, the value of the allocated class begins from 1.

data_refined1['Clustering Analysis'] = Predicting_kmeans.tolist() #now after the classes are incremently genetaed for the allocated class, we add the class named as Clustering Analysis( which holds the class values for allocated class) to our main dataframe.
data_refined1 #printing the dataframe with new added class as well as the actual class.

import plotly.express as px #importing plotly.express for better visual presentation.

#here we are genetaing two plots at the same time. The first plot is for the actual class, whereas the second plot is for allocated class.

#plotting the clusters based on actual class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined1, x='X1', y='X2',z='X3', color='Class', title= "Data Points According to Original Class") #passing the proper values for the actual class.
fig.show() #displaying the figure 1

#plotting the clusters based on allocated class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined1, x='X1', y='X2',z='X3',
                  color='Clustering Analysis',
                  title= "Data Points According to Class Allocated by Clustering Algorithm") #passing the proper values for the allocated class.
fig.show() #displaying the figure 2

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
result=accuracy_score(data_refined1['Class'], data_refined1['Clustering Analysis']) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", result*100, '%') #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined1['Class'],data_refined1['Clustering Analysis'])) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined1['Class'],data_refined1['Clustering Analysis'])) #printing classification report based on actual class and allocated class

"""**Note:** The validation purpose works well for supervised cases where classification can be easily achieved and is relaible. But, because Kmeans is a part of clustering techniques that comes under the unsupervised learning, it is, thu, difficult to achieve a complete validation. Moreover, everytime the accuracy will be also different because the clusters are generated differently everytime when executed and when we compare the allocated clustering class with the actual class, so the classes number differ and the values that are in class 1 according to the data might fall in class 3 of the allocated class. Thus, comparing class 1 and class 3 will never give efficient result because even if the values are forming in a specific cluster, still the class varies.

This is applicable for every other data. Only for the data 8, the accuracy will be 100% because as per the class- there is only one class in the entire dataset, and for clustering also we are taking total number of clusters as 1, so all the values lie is same class for the actual data and the allocated one. Thus, in that case we are comparing a class having one type with another class having same type of values in a cluster. Thus, accuarcy will be constant in case of data 8, but not in case of other datasets.

Also, random_state is not considered because the main aim is not to have a fixed accuracy by saving samples, rather the task focuses on understanding clustering prospective in comparision to the actual class.

---

## **Hierarchical Clustering for Data 1**
"""

#The second clustering technique performed is the Hierarchical Clustering Algorithm.
from sklearn.cluster import AgglomerativeClustering #importing essenatial libraries.
hclus=AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward') #taking a new variable hclus which represents Hierarchical Clustering and passing total number of clusters and other features in AgglomerativeClustering
y_hclus=hclus.fit_predict(Considered_Values) #assigning the prediction of Considered_Values to y_hclus

#The next part is the visualization part. Herw two plots are drawn, where one shows the clustering by Hierarchical algorithm, whereas the other plot shows the Dendrogram  Plot

import matplotlib.pyplot as plt #importing the required and essential libraries that are required for the plotting.
import scipy.cluster.hierarchy as shc

dendro = shc.dendrogram(shc.linkage(Considered_Values, method="ward", metric="euclidean")) #taking a new variable dendro and assigning the dendrogram plot foundation with by passing essential arguments required for plotting of the dendrogram.
plt.title("Dendrogram  Plot")  #naming the title
plt.ylabel("Distances")   #defining the x-axis
plt.xlabel("Classes")  #defining the y-axis
plt.show()  #displaying the figure 1


fig = plt.figure(figsize = (5,5)) #defining the size for figure 2
#project = fig.add_subplot(111, projection='3d') #if plot is 3D

plt.scatter(Considered_Values[y_hclus==0,0], Considered_Values[y_hclus==0,1], s=100, c='cyan') #plotting the cluster 1 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==1,0], Considered_Values[y_hclus==1,1], s=100, c='blue') #plotting the cluster 2 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==2,0], Considered_Values[y_hclus==2,1], s=100, c='red') #plotting the cluster 3 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==3,0], Considered_Values[y_hclus==3,1], s=100, c='green') #plotting the cluster 4 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==4,0], Considered_Values[y_hclus==4,1], s=100, c='yellow') #plotting the cluster 5 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==5,0], Considered_Values[y_hclus==5,1], s=100, c='purple') #plotting the cluster 6 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==6,0], Considered_Values[y_hclus==6,1], s=100, c='brown') #plotting the cluster 7 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.

#plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('Hierarchical Clustering Algorithm ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(m): #defining a new function add that takes m
  return m+1 #returning m+1
y_hclus=np.array(list(map(add,y_hclus))) #mapping the values of the allocated clustering class.

y_hclus #checking for values

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
hsresult=accuracy_score(data_refined1['Class'], y_hclus) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", hsresult*100, '%') #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined1['Class'],y_hclus)) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined1['Class'],y_hclus)) #printing classification report based on actual class and allocated class

"""# **For Data 2**"""

import pandas as pd #importing pandas
import numpy as np #importing numpy
from google.colab import files #importing files from local system
uploaded=files.upload() #shows the file uploaded from the local system.
import seaborn as sns  #importing seaborn for graphical purposes.
import matplotlib.pyplot as plt #importing matplotlib

data2=pd.read_csv('Data2.csv') #reading data1.csv from local system and assigning it to data1
data_refined2 = pd.DataFrame(data2, columns = ['X', 'Y', 'C', 'Class']) #reading columns 1,2,3 and 4 from the dataset and assigning it to a new variable data_refined1
data_refined2 = np.round(data_refined2, decimals = 3) #organizing the data by making all the values of same type by making all the values rounded to  3 decimal places.
data_refined2 #displaying data_refined2

Considered_Values= data_refined2.iloc[:, [0,1,2]].values #reading the values of columns and not considering the class and overall assigning it to Considered_Values. here, iloc is used for accessing the values based on selected columns.
Considered_Values #displaying Considered_Values

data_refined2.describe() #describing the refined data to understand highest, lowest and other related values in the columns. By this, we get a better understanding of how the data values in the dataset are separated and represented.

from sklearn.cluster import KMeans #importing kmeans
Understanding_kmeans = KMeans(n_clusters = 4, init = 'k-means++') #deciding number of clusters based on the classes of the data, and incrementing k++ and assigning the entire unit to new variable Understanding_kmeans
Predicting_kmeans = Understanding_kmeans.fit_predict(Considered_Values) #In this, we are predicting/ working with the kmeans algorithm using Understanding_kmeans.fit_predict and under this, Considered_Values is passed holds the column values of the main data.


fig = plt.figure(figsize = (9,9)) #deciding the size of the figure and assigning it to a new variable fig.
project = fig.add_subplot(111, projection='3d') #project variable holds the parameters for creating a 3D plot.


plt.scatter(Considered_Values[Predicting_kmeans == 0, 0], Considered_Values[Predicting_kmeans == 0, 1], s = 70, c = 'red', label = 'Cluster 1') #plotting the first cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 1, 0], Considered_Values[Predicting_kmeans == 1, 1], s = 70, c = 'blue', label = 'Cluster 2') #plotting the second cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 2, 0], Considered_Values[Predicting_kmeans == 2, 1], s = 70, c = 'green', label = 'Cluster 3') #plotting the third cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 3, 0], Considered_Values[Predicting_kmeans == 3, 1], s = 70, c = 'cyan', label = 'Cluster 4') #plotting the forth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 4, 0], Considered_Values[Predicting_kmeans == 4, 1], s = 70, c = 'magenta', label = 'Cluster 5') #plotting the fifth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 5, 0], Considered_Values[Predicting_kmeans == 5, 1], s = 70, c = 'orange', label = 'Cluster 6') #plotting the sixth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 6, 0], Considered_Values[Predicting_kmeans == 6, 1], s = 70, c = 'brown', label = 'Cluster 7') #plotting the seventh cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.

plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('K-Means Clustering General 3D ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(n): #defining a new function add that takes n
  return n+1 #returning n+1
Predicting_kmeans=np.array(list(map(add,Predicting_kmeans))) #mapping the values of the allocated clustering class.

# The main reason for doing the above code is because the classes given in the dataset are befinning from 1 but by default the classes that are generated for our training kmeans algorithm
# is beginning from 0 and because of this the comparision cannot be achieved for the actual classed and the allocated classes.
#Thus, to have similiar understanding of the actual classed and allocated classed, either the values of the class of actual classes should be decreased so that it begins from one
#or else the value of the allocated classes should be incremented by 1 (beginning form 1) so that there is a clear understanding.
#Thus, by above function, the value of the allocated class begins from 1.

data_refined2['Clustering Analysis'] = Predicting_kmeans.tolist() #now after the classes are incremently genetaed for the allocated class, we add the class named as Clustering Analysis( which holds the class values for allocated class) to our main dataframe.
data_refined2 #printing the dataframe with new added class as well as the actual class.

import plotly.express as px #importing plotly.express for better visual presentation.

#here we are genetaing two plots at the same time. The first plot is for the actual class, whereas the second plot is for allocated class.

#plotting the clusters based on actual class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined2, x='X', y='Y', z='C', color='Class', title= "Data Points According to Original Class") #passing the proper values for the actual class.
fig.show() #displaying the figure 1

#plotting the clusters based on allocated class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined2, x='X', y='Y', z='C', color='Clustering Analysis', title= "Data Points According to Class Allocated by Clustering Algorithm") #passing the proper values for the allocated class.
fig.show() #displaying the figure 2

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
result=accuracy_score(data_refined2['Class'], data_refined2['Clustering Analysis']) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", result*100 ,'%') #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined2['Class'],data_refined2['Clustering Analysis'])) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined2['Class'],data_refined2['Clustering Analysis'])) #printing classification report based on actual class and allocated class

"""## **Hierarchical Clustering for Data 2**"""

#The second clustering technique performed is the Hierarchical Clustering Algorithm.
from sklearn.cluster import AgglomerativeClustering #importing essenatial libraries.
hclus=AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward') #taking a new variable hclus which represents Hierarchical Clustering and passing total number of clusters and other features in AgglomerativeClustering
y_hclus=hclus.fit_predict(Considered_Values) #assigning the prediction of Considered_Values to y_hclus

#The next part is the visualization part. Herw two plots are drawn, where one shows the clustering by Hierarchical algorithm, whereas the other plot shows the Dendrogram  Plot

import matplotlib.pyplot as plt #importing the required and essential libraries that are required for the plotting.
import scipy.cluster.hierarchy as shc

dendro = shc.dendrogram(shc.linkage(Considered_Values, method="ward", metric="euclidean")) #taking a new variable dendro and assigning the dendrogram plot foundation with by passing essential arguments required for plotting of the dendrogram.
plt.title("Dendrogram  Plot")  #naming the title
plt.ylabel("Distances")   #defining the x-axis
plt.xlabel("Classes")  #defining the y-axis
plt.show()  #displaying the figure 1


fig = plt.figure(figsize = (5,5)) #defining the size for figure 2
#project = fig.add_subplot(111, projection='3d') #if plot is 3D

plt.scatter(Considered_Values[y_hclus==0,0], Considered_Values[y_hclus==0,1], s=100, c='cyan') #plotting the cluster 1 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==1,0], Considered_Values[y_hclus==1,1], s=100, c='blue') #plotting the cluster 2 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==2,0], Considered_Values[y_hclus==2,1], s=100, c='red') #plotting the cluster 3 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==3,0], Considered_Values[y_hclus==3,1], s=100, c='green') #plotting the cluster 4 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==4,0], Considered_Values[y_hclus==4,1], s=100, c='yellow') #plotting the cluster 5 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==5,0], Considered_Values[y_hclus==5,1], s=100, c='purple') #plotting the cluster 6 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==6,0], Considered_Values[y_hclus==6,1], s=100, c='brown') #plotting the cluster 7 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.

#plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('Hierarchical Clustering Algorithm ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(m): #defining a new function add that takes m
  return m+1 #returning m+1
y_hclus=np.array(list(map(add,y_hclus))) #mapping the values of the allocated clustering class.

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
hsresult=accuracy_score(data_refined2['Class'], y_hclus) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", hsresult*100, '%') #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined2['Class'],y_hclus)) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined2['Class'],y_hclus)) #printing classification report based on actual class and allocated class

"""# **For Data 3**"""

import pandas as pd #importing pandas
import numpy as np #importing numpy
from google.colab import files #importing files from local system
uploaded=files.upload() #shows the file uploaded from the local system.
import seaborn as sns  #importing seaborn for graphical purposes.
import matplotlib.pyplot as plt #importing matplotlib

data3=pd.read_csv('Data3.csv') #reading data3.csv from local system and assigning it to data1
data_refined3 = pd.DataFrame(data3, columns = ['X1', 'X2', 'X3', 'Class']) #reading columns 1,2,3 and 4 from the dataset and assigning it to a new variable data_refined1
data_refined3 = np.round(data_refined3, decimals = 3) #organizing the data by making all the values of same type by making all the values rounded to  3 decimal places.
data_refined3 #displaying data_refined3

Considered_Values= data_refined3.iloc[:, [0,1,2]].values #reading the values of columns and not considering the class and overall assigning it to Considered_Values. here, iloc is used for accessing the values based on selected columns.
Considered_Values #displaying Considered_Values

data_refined3.describe() #describing the refined data to understand highest, lowest and other related values in the columns. By this, we get a better understanding of how the data values in the dataset are separated and represented.

from sklearn.cluster import KMeans #importing kmeans
Understanding_kmeans = KMeans(n_clusters = 4, init = 'k-means++') #deciding number of clusters based on the classes of the data, and incrementing k++ and assigning the entire unit to new variable Understanding_kmeans
Predicting_kmeans = Understanding_kmeans.fit_predict(Considered_Values) #In this, we are predicting/ working with the kmeans algorithm using Understanding_kmeans.fit_predict and under this, Considered_Values is passed holds the column values of the main data.


fig = plt.figure(figsize = (10,10)) #deciding the size of the figure and assigning it to a new variable fig.
project = fig.add_subplot(111, projection='3d') #project variable holds the parameters for creating a 3D plot.


plt.scatter(Considered_Values[Predicting_kmeans == 0, 0], Considered_Values[Predicting_kmeans == 0, 1], s = 70, c = 'red', label = 'Cluster 1') #plotting the first cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 1, 0], Considered_Values[Predicting_kmeans == 1, 1], s = 70, c = 'blue', label = 'Cluster 2') #plotting the second cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 2, 0], Considered_Values[Predicting_kmeans == 2, 1], s = 70, c = 'green', label = 'Cluster 3') #plotting the third cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 3, 0], Considered_Values[Predicting_kmeans == 3, 1], s = 70, c = 'cyan', label = 'Cluster 4') #plotting the forth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 4, 0], Considered_Values[Predicting_kmeans == 4, 1], s = 70, c = 'magenta', label = 'Cluster 5') #plotting the fifth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 5, 0], Considered_Values[Predicting_kmeans == 5, 1], s = 70, c = 'orange', label = 'Cluster 6') #plotting the sixth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 6, 0], Considered_Values[Predicting_kmeans == 6, 1], s = 70, c = 'brown', label = 'Cluster 7') #plotting the seventh cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.

plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('K-Means Clustering General 3D ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(n): #defining a new function add that takes n
  return n+1 #returning n+1
Predicting_kmeans=np.array(list(map(add,Predicting_kmeans))) #mapping the values of the allocated clustering class.

# The main reason for doing the above code is because the classes given in the dataset are befinning from 1 but by default the classes that are generated for our training kmeans algorithm
# is beginning from 0 and because of this the comparision cannot be achieved for the actual classed and the allocated classes.
#Thus, to have similiar understanding of the actual classed and allocated classed, either the values of the class of actual classes should be decreased so that it begins from one
#or else the value of the allocated classes should be incremented by 1 (beginning form 1) so that there is a clear understanding.
#Thus, by above function, the value of the allocated class begins from 1.

data_refined3['Clustering Analysis'] = Predicting_kmeans.tolist() #now after the classes are incremently genetaed for the allocated class, we add the class named as Clustering Analysis( which holds the class values for allocated class) to our main dataframe.
data_refined3 #printing the dataframe with new added class as well as the actual class.

import plotly.express as px #importing plotly.express for better visual presentation.

#here we are genetaing two plots at the same time. The first plot is for the actual class, whereas the second plot is for allocated class.

#plotting the clusters based on actual class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined3, x='X1', y='X2', z='X3', color='Class', title= "Data Points According to Original Class") #passing the proper values for the actual class.
fig.show() #displaying the figure 1

#plotting the clusters based on allocated class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined3, x='X1', y='X2', z='X3', color='Clustering Analysis', title= "Data Points According to Class Allocated by Clustering Algorithm") #passing the proper values for the allocated class.
fig.show() #displaying the figure 2

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
result=accuracy_score(data_refined3['Class'], data_refined3['Clustering Analysis']) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", result*100, '%') #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined3['Class'],data_refined3['Clustering Analysis'])) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined3['Class'],data_refined3['Clustering Analysis'])) #printing classification report based on actual class and allocated class

"""## **Hierarchical Clustering for Data 3**"""

#The second clustering technique performed is the Hierarchical Clustering Algorithm.
from sklearn.cluster import AgglomerativeClustering #importing essenatial libraries.
hclus=AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward') #taking a new variable hclus which represents Hierarchical Clustering and passing total number of clusters and other features in AgglomerativeClustering
y_hclus=hclus.fit_predict(Considered_Values) #assigning the prediction of Considered_Values to y_hclus

#The next part is the visualization part. Herw two plots are drawn, where one shows the clustering by Hierarchical algorithm, whereas the other plot shows the Dendrogram  Plot

import matplotlib.pyplot as plt #importing the required and essential libraries that are required for the plotting.
import scipy.cluster.hierarchy as shc

dendro = shc.dendrogram(shc.linkage(Considered_Values, method="ward", metric="euclidean")) #taking a new variable dendro and assigning the dendrogram plot foundation with by passing essential arguments required for plotting of the dendrogram.
plt.title("Dendrogram  Plot")  #naming the title
plt.ylabel("Distances")   #defining the x-axis
plt.xlabel("Classes")  #defining the y-axis
plt.show()  #displaying the figure 1


fig = plt.figure(figsize = (5,5)) #defining the size for figure 2
#project = fig.add_subplot(111, projection='3d') #if plot is 3D

plt.scatter(Considered_Values[y_hclus==0,0], Considered_Values[y_hclus==0,1], s=100, c='orange', label = 'Cluster 1') #plotting the cluster 1 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==1,0], Considered_Values[y_hclus==1,1], s=100, c='blue', label = 'Cluster 2') #plotting the cluster 2 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==2,0], Considered_Values[y_hclus==2,1], s=100, c='red', label = 'Cluster 3') #plotting the cluster 3 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==3,0], Considered_Values[y_hclus==3,1], s=100, c='green',label = 'Cluster 4') #plotting the cluster 4 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==4,0], Considered_Values[y_hclus==4,1], s=100, c='yellow',label = 'Cluster 5') #plotting the cluster 5 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==5,0], Considered_Values[y_hclus==5,1], s=100, c='purple', label = 'Cluster 6') #plotting the cluster 6 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==6,0], Considered_Values[y_hclus==6,1], s=100, c='brown', label = 'Cluster 7') #plotting the cluster 7 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.

#plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('Hierarchical Clustering Algorithm ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(m): #defining a new function add that takes m
  return m+1 #returning m+1
y_hclus=np.array(list(map(add,y_hclus))) #mapping the values of the allocated clustering class.

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
hsresult=accuracy_score(data_refined3['Class'], y_hclus) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", hsresult*100, '%') #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined3['Class'],y_hclus)) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined3['Class'],y_hclus)) #printing classification report based on actual class and allocated class

"""# **For Data 4**"""

import pandas as pd #importing pandas
import numpy as np #importing numpy
from google.colab import files #importing files from local system
uploaded=files.upload() #shows the file uploaded from the local system.
import seaborn as sns  #importing seaborn for graphical purposes.
import matplotlib.pyplot as plt #importing matplotlib

data4=pd.read_csv('Data4.csv') #reading data4.csv from local system and assigning it to data1
data_refined4 = pd.DataFrame(data4, columns = ['X1', 'X2', 'X3', 'Class']) #reading columns 1,2,3 and 4 from the dataset and assigning it to a new variable data_refined1
data_refined3 = np.round(data_refined4, decimals = 3) #organizing the data by making all the values of same type by making all the values rounded to  3 decimal places.
data_refined4 #displaying data_refined4

Considered_Values= data_refined4.iloc[:, [0,1,2]].values #reading the values of columns and not considering the class and overall assigning it to Considered_Values. here, iloc is used for accessing the values based on selected columns.
Considered_Values #displaying Considered_Values

data_refined4.describe() #describing the refined data to understand highest, lowest and other related values in the columns. By this, we get a better understanding of how the data values in the dataset are separated and represented.

from sklearn.cluster import KMeans #importing kmeans
Understanding_kmeans = KMeans(n_clusters = 2, init = 'k-means++') #deciding number of clusters based on the classes of the data, and incrementing k++ and assigning the entire unit to new variable Understanding_kmeans
Predicting_kmeans = Understanding_kmeans.fit_predict(Considered_Values) #In this, we are predicting/ working with the kmeans algorithm using Understanding_kmeans.fit_predict and under this, Considered_Values is passed holds the column values of the main data.


fig = plt.figure(figsize = (10,10)) #deciding the size of the figure and assigning it to a new variable fig.
project = fig.add_subplot(111, projection='3d') #project variable holds the parameters for creating a 3D plot.


plt.scatter(Considered_Values[Predicting_kmeans == 0, 0], Considered_Values[Predicting_kmeans == 0, 1], s = 70, c = 'red', label = 'Cluster 1') #plotting the first cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 1, 0], Considered_Values[Predicting_kmeans == 1, 1], s = 70, c = 'blue', label = 'Cluster 2') #plotting the second cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 2, 0], Considered_Values[Predicting_kmeans == 2, 1], s = 70, c = 'green', label = 'Cluster 3') #plotting the third cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 3, 0], Considered_Values[Predicting_kmeans == 3, 1], s = 70, c = 'cyan', label = 'Cluster 4') #plotting the forth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 4, 0], Considered_Values[Predicting_kmeans == 4, 1], s = 70, c = 'magenta', label = 'Cluster 5') #plotting the fifth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 5, 0], Considered_Values[Predicting_kmeans == 5, 1], s = 70, c = 'orange', label = 'Cluster 6') #plotting the sixth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 6, 0], Considered_Values[Predicting_kmeans == 6, 1], s = 70, c = 'brown', label = 'Cluster 7') #plotting the seventh cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.

plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('K-Means Clustering General 3D ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(n): #defining a new function add that takes n
  return n+1 #returning n+1
Predicting_kmeans=np.array(list(map(add,Predicting_kmeans))) #mapping the values of the allocated clustering class.

# The main reason for doing the above code is because the classes given in the dataset are befinning from 1 but by default the classes that are generated for our training kmeans algorithm
# is beginning from 0 and because of this the comparision cannot be achieved for the actual classed and the allocated classes.
#Thus, to have similiar understanding of the actual classed and allocated classed, either the values of the class of actual classes should be decreased so that it begins from one
#or else the value of the allocated classes should be incremented by 1 (beginning form 1) so that there is a clear understanding.
#Thus, by above function, the value of the allocated class begins from 1.

data_refined4['Clustering Analysis'] = Predicting_kmeans.tolist() #now after the classes are incremently genetaed for the allocated class, we add the class named as Clustering Analysis( which holds the class values for allocated class) to our main dataframe.
data_refined4 #printing the dataframe with new added class as well as the actual class.

import plotly.express as px #importing plotly.express for better visual presentation.

#here we are genetaing two plots at the same time. The first plot is for the actual class, whereas the second plot is for allocated class.

#plotting the clusters based on actual class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined4, x='X1', y='X2', z='X3', color='Class', title= "Data Points According to Original Class") #passing the proper values for the actual class.
fig.show() #displaying the figure 1

#plotting the clusters based on allocated class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined4, x='X1', y='X2', z='X3', color='Clustering Analysis', title= "Data Points According to Class Allocated by Clustering Algorithm") #passing the proper values for the allocated class.
fig.show() #displaying the figure 2

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
result=accuracy_score(data_refined4['Class'], data_refined4['Clustering Analysis']) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", result*100,"%") #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined4['Class'],data_refined4['Clustering Analysis'])) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined4['Class'],data_refined4['Clustering Analysis'])) #printing classification report based on actual class and allocated class

"""## **Hierarchical Clustering for Data 4**"""

#The second clustering technique performed is the Hierarchical Clustering Algorithm.
from sklearn.cluster import AgglomerativeClustering #importing essenatial libraries.
hclus=AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward') #taking a new variable hclus which represents Hierarchical Clustering and passing total number of clusters and other features in AgglomerativeClustering
y_hclus=hclus.fit_predict(Considered_Values) #assigning the prediction of Considered_Values to y_hclus

#The next part is the visualization part. Herw two plots are drawn, where one shows the clustering by Hierarchical algorithm, whereas the other plot shows the Dendrogram  Plot

import matplotlib.pyplot as plt #importing the required and essential libraries that are required for the plotting.
import scipy.cluster.hierarchy as shc

dendro = shc.dendrogram(shc.linkage(Considered_Values, method="ward", metric="euclidean")) #taking a new variable dendro and assigning the dendrogram plot foundation with by passing essential arguments required for plotting of the dendrogram.
plt.title("Dendrogram  Plot")  #naming the title
plt.ylabel("Distances")   #defining the x-axis
plt.xlabel("Classes")  #defining the y-axis
plt.show()  #displaying the figure 1


fig = plt.figure(figsize = (5,5)) #defining the size for figure 2
#project = fig.add_subplot(111, projection='3d') #if plot is 3D

plt.scatter(Considered_Values[y_hclus==0,0], Considered_Values[y_hclus==0,1], s=100, c='cyan', label = 'Cluster 1') #plotting the cluster 1 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==1,0], Considered_Values[y_hclus==1,1], s=100, c='blue', label = 'Cluster 2') #plotting the cluster 2 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==2,0], Considered_Values[y_hclus==2,1], s=100, c='red', label = 'Cluster 3') #plotting the cluster 3 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==3,0], Considered_Values[y_hclus==3,1], s=100, c='green',label = 'Cluster 4') #plotting the cluster 4 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# # plt.scatter(Considered_Values[y_hclus==4,0], Considered_Values[y_hclus==4,1], s=100, c='yellow',label = 'Cluster 5') #plotting the cluster 5 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==5,0], Considered_Values[y_hclus==5,1], s=100, c='purple', label = 'Cluster 6') #plotting the cluster 6 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==6,0], Considered_Values[y_hclus==6,1], s=100, c='brown', label = 'Cluster 7') #plotting the cluster 7 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.

#plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('Hierarchical Clustering Algorithm ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(m): #defining a new function add that takes m
  return m+1 #returning m+1
y_hclus=np.array(list(map(add,y_hclus))) #mapping the values of the allocated clustering class.

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
hsresult=accuracy_score(data_refined4['Class'], y_hclus) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", hsresult*100, '%') #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined4['Class'],y_hclus)) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined4['Class'],y_hclus)) #printing classification report based on actual class and allocated class

"""# **For Data 5**"""

import pandas as pd #importing pandas
import numpy as np #importing numpy
from google.colab import files #importing files from local system
uploaded=files.upload() #shows the file uploaded from the local system.
import seaborn as sns  #importing seaborn for graphical purposes.
import matplotlib.pyplot as plt #importing matplotlib

data5=pd.read_csv('Data5.csv') #reading data5.csv from local system and assigning it to data1
data_refined5 = pd.DataFrame(data5, columns = ['X1', 'X2', 'X3', 'Class']) #reading columns 1,2,3 and 4 from the dataset and assigning it to a new variable data_refined1
data_refined5 = np.round(data_refined5, decimals = 3) #organizing the data by making all the values of same type by making all the values rounded to  3 decimal places.
data_refined5 #displaying data_refined5

Considered_Values= data_refined5.iloc[:, [0,1,2]].values #reading the values of columns and not considering the class and overall assigning it to Considered_Values. here, iloc is used for accessing the values based on selected columns.
Considered_Values #displaying Considered_Values

data_refined5.describe() #describing the refined data to understand highest, lowest and other related values in the columns. By this, we get a better understanding of how the data values in the dataset are separated and represented.

from sklearn.cluster import KMeans #importing kmeans
Understanding_kmeans = KMeans(n_clusters = 2, init = 'k-means++') #deciding number of clusters based on the classes of the data, and incrementing k++ and assigning the entire unit to new variable Understanding_kmeans
Predicting_kmeans = Understanding_kmeans.fit_predict(Considered_Values) #In this, we are predicting/ working with the kmeans algorithm using Understanding_kmeans.fit_predict and under this, Considered_Values is passed holds the column values of the main data.


fig = plt.figure(figsize = (7,7)) #deciding the size of the figure and assigning it to a new variable fig.
#project = fig.add_subplot(111, projection='3d') #project variable holds the parameters for creating a 3D plot.


plt.scatter(Considered_Values[Predicting_kmeans == 0, 0], Considered_Values[Predicting_kmeans == 0, 1], s = 70, c = 'red', label = 'Cluster 1') #plotting the first cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 1, 0], Considered_Values[Predicting_kmeans == 1, 1], s = 70, c = 'blue', label = 'Cluster 2') #plotting the second cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 2, 0], Considered_Values[Predicting_kmeans == 2, 1], s = 70, c = 'green', label = 'Cluster 3') #plotting the third cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 3, 0], Considered_Values[Predicting_kmeans == 3, 1], s = 70, c = 'cyan', label = 'Cluster 4') #plotting the forth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 4, 0], Considered_Values[Predicting_kmeans == 4, 1], s = 70, c = 'magenta', label = 'Cluster 5') #plotting the fifth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 5, 0], Considered_Values[Predicting_kmeans == 5, 1], s = 70, c = 'orange', label = 'Cluster 6') #plotting the sixth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 6, 0], Considered_Values[Predicting_kmeans == 6, 1], s = 70, c = 'brown', label = 'Cluster 7') #plotting the seventh cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.

plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 80, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('K-Means Clustering General 3D ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(n): #defining a new function add that takes n
  return n+1 #returning n+1
Predicting_kmeans=np.array(list(map(add,Predicting_kmeans))) #mapping the values of the allocated clustering class.

# The main reason for doing the above code is because the classes given in the dataset are befinning from 1 but by default the classes that are generated for our training kmeans algorithm
# is beginning from 0 and because of this the comparision cannot be achieved for the actual classed and the allocated classes.
#Thus, to have similiar understanding of the actual classed and allocated classed, either the values of the class of actual classes should be decreased so that it begins from one
#or else the value of the allocated classes should be incremented by 1 (beginning form 1) so that there is a clear understanding.
#Thus, by above function, the value of the allocated class begins from 1.

data_refined5['Clustering Analysis'] = Predicting_kmeans.tolist() #now after the classes are incremently genetaed for the allocated class, we add the class named as Clustering Analysis( which holds the class values for allocated class) to our main dataframe.
data_refined5 #printing the dataframe with new added class as well as the actual class.

import plotly.express as px #importing plotly.express for better visual presentation.

#here we are genetaing two plots at the same time. The first plot is for the actual class, whereas the second plot is for allocated class.

#plotting the clusters based on actual class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined5, x='X1', y='X2', z='X3', color='Class', title= "Data Points According to Original Class") #passing the proper values for the actual class.
fig.show() #displaying the figure 1

#plotting the clusters based on allocated class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined5, x='X1', y='X2', z='X3', color='Clustering Analysis', title= "Data Points According to Class Allocated by Clustering Algorithm") #passing the proper values for the allocated class.
fig.show() #displaying the figure 2

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
result=accuracy_score(data_refined5['Class'], data_refined5['Clustering Analysis']) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", result*100,"%") #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined5['Class'],data_refined5['Clustering Analysis'])) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined5['Class'],data_refined5['Clustering Analysis'])) #printing classification report based on actual class and allocated class

"""## **Hierarchical Clustering for Data 5**"""

#The second clustering technique performed is the Hierarchical Clustering Algorithm.
from sklearn.cluster import AgglomerativeClustering #importing essenatial libraries.
hclus=AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward') #taking a new variable hclus which represents Hierarchical Clustering and passing total number of clusters and other features in AgglomerativeClustering
y_hclus=hclus.fit_predict(Considered_Values) #assigning the prediction of Considered_Values to y_hclus

#The next part is the visualization part. Herw two plots are drawn, where one shows the clustering by Hierarchical algorithm, whereas the other plot shows the Dendrogram  Plot

import matplotlib.pyplot as plt #importing the required and essential libraries that are required for the plotting.
import scipy.cluster.hierarchy as shc

dendro = shc.dendrogram(shc.linkage(Considered_Values, method="ward", metric="euclidean")) #taking a new variable dendro and assigning the dendrogram plot foundation with by passing essential arguments required for plotting of the dendrogram.
plt.title("Dendrogram  Plot")  #naming the title
plt.ylabel("Distances")   #defining the x-axis
plt.xlabel("Classes")  #defining the y-axis
plt.show()  #displaying the figure 1


fig = plt.figure(figsize = (5,5)) #defining the size for figure 2
#project = fig.add_subplot(111, projection='3d') #if plot is 3D

plt.scatter(Considered_Values[y_hclus==0,0], Considered_Values[y_hclus==0,1], s=100, c='cyan', label = 'Cluster 1') #plotting the cluster 1 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==1,0], Considered_Values[y_hclus==1,1], s=100, c='blue', label = 'Cluster 2') #plotting the cluster 2 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==2,0], Considered_Values[y_hclus==2,1], s=100, c='red', label = 'Cluster 3') #plotting the cluster 3 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==3,0], Considered_Values[y_hclus==3,1], s=100, c='green',label = 'Cluster 4') #plotting the cluster 4 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# # plt.scatter(Considered_Values[y_hclus==4,0], Considered_Values[y_hclus==4,1], s=100, c='yellow',label = 'Cluster 5') #plotting the cluster 5 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==5,0], Considered_Values[y_hclus==5,1], s=100, c='purple', label = 'Cluster 6') #plotting the cluster 6 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==6,0], Considered_Values[y_hclus==6,1], s=100, c='brown', label = 'Cluster 7') #plotting the cluster 7 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.

#plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('Hierarchical Clustering Algorithm ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(m): #defining a new function add that takes m
  return m+1 #returning m+1
y_hclus=np.array(list(map(add,y_hclus))) #mapping the values of the allocated clustering class.

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
hsresult=accuracy_score(data_refined5['Class'], y_hclus) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", hsresult*100, '%') #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined5['Class'],y_hclus)) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined5['Class'],y_hclus)) #printing classification report based on actual class and allocated class

"""# **For Data 6**"""

import pandas as pd #importing pandas
import numpy as np #importing numpy
from google.colab import files #importing files from local system
uploaded=files.upload() #shows the file uploaded from the local system.
import seaborn as sns  #importing seaborn for graphical purposes.
import matplotlib.pyplot as plt #importing matplotlib

data6=pd.read_csv('Data6.csv') #reading data6.csv from local system and assigning it to data1
data_refined6 = pd.DataFrame(data6, columns = ['X1', 'X2', 'Class']) #reading columns 1,2,3 and 4 from the dataset and assigning it to a new variable data_refined1
data_refined6 = np.round(data_refined6, decimals = 2) #organizing the data by making all the values of same type by making all the values rounded to  3 decimal places.
data_refined6 #displaying data_refined6

Considered_Values= data_refined6.iloc[:, [0,1]].values #reading the values of columns and not considering the class and overall assigning it to Considered_Values. here, iloc is used for accessing the values based on selected columns.
Considered_Values #displaying Considered_Values

data_refined6.describe() #describing the refined data to understand highest, lowest and other related values in the columns. By this, we get a better understanding of how the data values in the dataset are separated and represented.

from sklearn.cluster import KMeans #importing kmeans
Understanding_kmeans = KMeans(n_clusters = 2, init = 'k-means++') #deciding number of clusters based on the classes of the data, and incrementing k++ and assigning the entire unit to new variable Understanding_kmeans
Predicting_kmeans = Understanding_kmeans.fit_predict(Considered_Values) #In this, we are predicting/ working with the kmeans algorithm using Understanding_kmeans.fit_predict and under this, Considered_Values is passed holds the column values of the main data.


fig = plt.figure(figsize = (7,7)) #deciding the size of the figure and assigning it to a new variable fig.
#project = fig.add_subplot(111, projection='3d') #project variable holds the parameters for creating a 3D plot.


plt.scatter(Considered_Values[Predicting_kmeans == 0, 0], Considered_Values[Predicting_kmeans == 0, 1], s = 70, c = 'red', label = 'Cluster 1') #plotting the first cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 1, 0], Considered_Values[Predicting_kmeans == 1, 1], s = 70, c = 'blue', label = 'Cluster 2') #plotting the second cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 2, 0], Considered_Values[Predicting_kmeans == 2, 1], s = 70, c = 'green', label = 'Cluster 3') #plotting the third cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 3, 0], Considered_Values[Predicting_kmeans == 3, 1], s = 70, c = 'cyan', label = 'Cluster 4') #plotting the forth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 4, 0], Considered_Values[Predicting_kmeans == 4, 1], s = 70, c = 'magenta', label = 'Cluster 5') #plotting the fifth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 5, 0], Considered_Values[Predicting_kmeans == 5, 1], s = 70, c = 'orange', label = 'Cluster 6') #plotting the sixth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 6, 0], Considered_Values[Predicting_kmeans == 6, 1], s = 70, c = 'brown', label = 'Cluster 7') #plotting the seventh cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.

plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 80, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('K-Means Clustering General 3D ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(n): #defining a new function add that takes n
  return n+1 #returning n+1
Predicting_kmeans=np.array(list(map(add,Predicting_kmeans))) #mapping the values of the allocated clustering class.

# The main reason for doing the above code is because the classes given in the dataset are befinning from 1 but by default the classes that are generated for our training kmeans algorithm
# is beginning from 0 and because of this the comparision cannot be achieved for the actual classed and the allocated classes.
#Thus, to have similiar understanding of the actual classed and allocated classed, either the values of the class of actual classes should be decreased so that it begins from one
#or else the value of the allocated classes should be incremented by 1 (beginning form 1) so that there is a clear understanding.
#Thus, by above function, the value of the allocated class begins from 1.

data_refined6['Clustering Analysis'] = Predicting_kmeans.tolist() #now after the classes are incremently genetaed for the allocated class, we add the class named as Clustering Analysis( which holds the class values for allocated class) to our main dataframe.
data_refined6 #printing the dataframe with new added class as well as the actual class.

import plotly.express as px #importing plotly.express for better visual presentation.

#here we are genetaing two plots at the same time. The first plot is for the actual class, whereas the second plot is for allocated class.

#plotting the clusters based on actual class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter(data_refined6, x='X1', y='X2', color='Class', title= "Data Points According to Original Class") #passing the proper values for the actual class.
fig.show() #displaying the figure 1

#plotting the clusters based on allocated class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter(data_refined6, x='X1', y='X2', color='Clustering Analysis', title= "Data Points According to Class Allocated by Clustering Algorithm") #passing the proper values for the allocated class.
fig.show() #displaying the figure 2

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
result=accuracy_score(data_refined6['Class'], data_refined6['Clustering Analysis']) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", result*100,"%") #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined6['Class'],data_refined6['Clustering Analysis'])) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined6['Class'],data_refined6['Clustering Analysis'])) #printing classification report based on actual class and allocated class

"""## **Hierarchical Clustering for Data 6**"""

#The second clustering technique performed is the Hierarchical Clustering Algorithm.
from sklearn.cluster import AgglomerativeClustering #importing essenatial libraries.
hclus=AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward') #taking a new variable hclus which represents Hierarchical Clustering and passing total number of clusters and other features in AgglomerativeClustering
y_hclus=hclus.fit_predict(Considered_Values) #assigning the prediction of Considered_Values to y_hclus

#The next part is the visualization part. Herw two plots are drawn, where one shows the clustering by Hierarchical algorithm, whereas the other plot shows the Dendrogram  Plot

import matplotlib.pyplot as plt #importing the required and essential libraries that are required for the plotting.
import scipy.cluster.hierarchy as shc

dendro = shc.dendrogram(shc.linkage(Considered_Values, method="ward", metric="euclidean")) #taking a new variable dendro and assigning the dendrogram plot foundation with by passing essential arguments required for plotting of the dendrogram.
plt.title("Dendrogram  Plot")  #naming the title
plt.ylabel("Distances")   #defining the x-axis
plt.xlabel("Classes")  #defining the y-axis
plt.show()  #displaying the figure 1


fig = plt.figure(figsize = (5,5)) #defining the size for figure 2
#project = fig.add_subplot(111, projection='3d') #if plot is 3D

plt.scatter(Considered_Values[y_hclus==0,0], Considered_Values[y_hclus==0,1], s=100, c='cyan', label = 'Cluster 1') #plotting the cluster 1 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==1,0], Considered_Values[y_hclus==1,1], s=100, c='blue', label = 'Cluster 2') #plotting the cluster 2 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==2,0], Considered_Values[y_hclus==2,1], s=100, c='red', label = 'Cluster 3') #plotting the cluster 3 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==3,0], Considered_Values[y_hclus==3,1], s=100, c='green',label = 'Cluster 4') #plotting the cluster 4 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# # plt.scatter(Considered_Values[y_hclus==4,0], Considered_Values[y_hclus==4,1], s=100, c='yellow',label = 'Cluster 5') #plotting the cluster 5 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==5,0], Considered_Values[y_hclus==5,1], s=100, c='purple', label = 'Cluster 6') #plotting the cluster 6 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==6,0], Considered_Values[y_hclus==6,1], s=100, c='brown', label = 'Cluster 7') #plotting the cluster 7 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.

#plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('Hierarchical Clustering Algorithm ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(m): #defining a new function add that takes m
  return m+1 #returning m+1
y_hclus=np.array(list(map(add,y_hclus))) #mapping the values of the allocated clustering class.

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
hsresult=accuracy_score(data_refined6['Class'], y_hclus) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", hsresult*100, '%') #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined6['Class'],y_hclus)) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined6['Class'],y_hclus)) #printing classification report based on actual class and allocated class

"""# **For Data 7**"""

import pandas as pd #importing pandas
import numpy as np #importing numpy
from google.colab import files #importing files from local system
uploaded=files.upload() #shows the file uploaded from the local system.
import seaborn as sns  #importing seaborn for graphical purposes.
import matplotlib.pyplot as plt #importing matplotlib

data7=pd.read_csv('Data7.csv') #reading data7.csv from local system and assigning it to data1
data_refined7 = pd.DataFrame(data7, columns = ['X1', 'X2', 'Class']) #reading columns 1,2,3 and 4 from the dataset and assigning it to a new variable data_refined1
data_refined7 = np.round(data_refined7, decimals = 2) #organizing the data by making all the values of same type by making all the values rounded to  3 decimal places.
data_refined7 #displaying data_refined7

Considered_Values= data_refined7.iloc[:, [0,1]].values #reading the values of columns and not considering the class and overall assigning it to Considered_Values. here, iloc is used for accessing the values based on selected columns.
Considered_Values #displaying Considered_Values

data_refined7.describe() #describing the refined data to understand highest, lowest and other related values in the columns. By this, we get a better understanding of how the data values in the dataset are separated and represented.

from sklearn.cluster import KMeans #importing kmeans
Understanding_kmeans = KMeans(n_clusters = 6, init = 'k-means++') #deciding number of clusters based on the classes of the data, and incrementing k++ and assigning the entire unit to new variable Understanding_kmeans
Predicting_kmeans = Understanding_kmeans.fit_predict(Considered_Values) #In this, we are predicting/ working with the kmeans algorithm using Understanding_kmeans.fit_predict and under this, Considered_Values is passed holds the column values of the main data.


fig = plt.figure(figsize = (7,7)) #deciding the size of the figure and assigning it to a new variable fig.
#project = fig.add_subplot(111, projection='3d') #project variable holds the parameters for creating a 3D plot.


plt.scatter(Considered_Values[Predicting_kmeans == 0, 0], Considered_Values[Predicting_kmeans == 0, 1], s = 70, c = 'red', label = 'Cluster 1') #plotting the first cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 1, 0], Considered_Values[Predicting_kmeans == 1, 1], s = 70, c = 'blue', label = 'Cluster 2') #plotting the second cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 2, 0], Considered_Values[Predicting_kmeans == 2, 1], s = 70, c = 'green', label = 'Cluster 3') #plotting the third cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 3, 0], Considered_Values[Predicting_kmeans == 3, 1], s = 70, c = 'cyan', label = 'Cluster 4') #plotting the forth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 4, 0], Considered_Values[Predicting_kmeans == 4, 1], s = 70, c = 'magenta', label = 'Cluster 5') #plotting the fifth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.scatter(Considered_Values[Predicting_kmeans == 5, 0], Considered_Values[Predicting_kmeans == 5, 1], s = 70, c = 'orange', label = 'Cluster 6') #plotting the sixth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 6, 0], Considered_Values[Predicting_kmeans == 6, 1], s = 70, c = 'brown', label = 'Cluster 7') #plotting the seventh cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.

plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 80, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('K-Means Clustering General 3D ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(n): #defining a new function add that takes n
  return n+1 #returning n+1
Predicting_kmeans=np.array(list(map(add,Predicting_kmeans))) #mapping the values of the allocated clustering class.

# The main reason for doing the above code is because the classes given in the dataset are befinning from 1 but by default the classes that are generated for our training kmeans algorithm
# is beginning from 0 and because of this the comparision cannot be achieved for the actual classed and the allocated classes.
#Thus, to have similiar understanding of the actual classed and allocated classed, either the values of the class of actual classes should be decreased so that it begins from one
#or else the value of the allocated classes should be incremented by 1 (beginning form 1) so that there is a clear understanding.
#Thus, by above function, the value of the allocated class begins from 1.

data_refined7['Clustering Analysis'] = Predicting_kmeans.tolist() #now after the classes are incremently genetaed for the allocated class, we add the class named as Clustering Analysis( which holds the class values for allocated class) to our main dataframe.
data_refined7 #printing the dataframe with new added class as well as the actual class.

import plotly.express as px #importing plotly.express for better visual presentation.

#here we are genetaing two plots at the same time. The first plot is for the actual class, whereas the second plot is for allocated class.

#plotting the clusters based on actual class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter(data_refined7, x='X1', y='X2', color='Class', title= "Data Points According to Original Class") #passing the proper values for the actual class.
fig.show() #displaying the figure 1

#plotting the clusters based on allocated class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter(data_refined7, x='X1', y='X2', color='Clustering Analysis', title= "Data Points According to Class Allocated by Clustering Algorithm") #passing the proper values for the allocated class.
fig.show() #displaying the figure 2

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
result=accuracy_score(data_refined7['Class'], data_refined7['Clustering Analysis']) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", result*100,"%") #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined7['Class'],data_refined7['Clustering Analysis'])) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined7['Class'],data_refined7['Clustering Analysis'])) #printing classification report based on actual class and allocated class

"""## **Hierarchical Clustering for Data 7**"""

#The second clustering technique performed is the Hierarchical Clustering Algorithm.
from sklearn.cluster import AgglomerativeClustering #importing essenatial libraries.
hclus=AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward') #taking a new variable hclus which represents Hierarchical Clustering and passing total number of clusters and other features in AgglomerativeClustering
y_hclus=hclus.fit_predict(Considered_Values) #assigning the prediction of Considered_Values to y_hclus

#The next part is the visualization part. Herw two plots are drawn, where one shows the clustering by Hierarchical algorithm, whereas the other plot shows the Dendrogram  Plot

import matplotlib.pyplot as plt #importing the required and essential libraries that are required for the plotting.
import scipy.cluster.hierarchy as shc

dendro = shc.dendrogram(shc.linkage(Considered_Values, method="ward", metric="euclidean")) #taking a new variable dendro and assigning the dendrogram plot foundation with by passing essential arguments required for plotting of the dendrogram.
plt.title("Dendrogram  Plot")  #naming the title
plt.ylabel("Distances")   #defining the x-axis
plt.xlabel("Classes")  #defining the y-axis
plt.show()  #displaying the figure 1


fig = plt.figure(figsize = (5,5)) #defining the size for figure 2
#project = fig.add_subplot(111, projection='3d') #if plot is 3D

plt.scatter(Considered_Values[y_hclus==0,0], Considered_Values[y_hclus==0,1], s=100, c='cyan', label = 'Cluster 1') #plotting the cluster 1 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==1,0], Considered_Values[y_hclus==1,1], s=100, c='blue', label = 'Cluster 2') #plotting the cluster 2 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==2,0], Considered_Values[y_hclus==2,1], s=100, c='red', label = 'Cluster 3') #plotting the cluster 3 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==3,0], Considered_Values[y_hclus==3,1], s=100, c='green',label = 'Cluster 4') #plotting the cluster 4 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==4,0], Considered_Values[y_hclus==4,1], s=100, c='yellow',label = 'Cluster 5') #plotting the cluster 5 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
plt.scatter(Considered_Values[y_hclus==5,0], Considered_Values[y_hclus==5,1], s=100, c='purple', label = 'Cluster 6') #plotting the cluster 6 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==6,0], Considered_Values[y_hclus==6,1], s=100, c='brown', label = 'Cluster 7') #plotting the cluster 7 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.

#plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('Hierarchical Clustering Algorithm ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(m): #defining a new function add that takes m
  return m+1 #returning m+1
y_hclus=np.array(list(map(add,y_hclus))) #mapping the values of the allocated clustering class.

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
hsresult=accuracy_score(data_refined7['Class'], y_hclus) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", hsresult*100, '%') #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined7['Class'],y_hclus)) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined7['Class'],y_hclus)) #printing classification report based on actual class and allocated class

"""# **For Data 8**"""

import pandas as pd #importing pandas
import numpy as np #importing numpy
from google.colab import files #importing files from local system
uploaded=files.upload() #shows the file uploaded from the local system.
import seaborn as sns  #importing seaborn for graphical purposes.
import matplotlib.pyplot as plt #importing matplotlib

data8=pd.read_csv('Data8.csv') #reading data8.csv from local system and assigning it to data1
data_refined8 = pd.DataFrame(data8, columns = ['X1', 'X2','X3','Class']) #reading columns 1,2,3 and 4 from the dataset and assigning it to a new variable data_refined1
data_refined8 = np.round(data_refined8, decimals = 2) #organizing the data by making all the values of same type by making all the values rounded to  3 decimal places.
data_refined8 #displaying data_refined8

Considered_Values= data_refined8.iloc[:, [0,1,2]].values #reading the values of columns and not considering the class and overall assigning it to Considered_Values. here, iloc is used for accessing the values based on selected columns.
Considered_Values #displaying Considered_Values

data_refined8.describe() #describing the refined data to understand highest, lowest and other related values in the columns. By this, we get a better understanding of how the data values in the dataset are separated and represented.

from sklearn.cluster import KMeans #importing kmeans
Understanding_kmeans = KMeans(n_clusters = 1, init = 'k-means++') #deciding number of clusters based on the classes of the data, and incrementing k++ and assigning the entire unit to new variable Understanding_kmeans
Predicting_kmeans = Understanding_kmeans.fit_predict(Considered_Values) #In this, we are predicting/ working with the kmeans algorithm using Understanding_kmeans.fit_predict and under this, Considered_Values is passed holds the column values of the main data.


fig = plt.figure(figsize = (7,7)) #deciding the size of the figure and assigning it to a new variable fig.
#project = fig.add_subplot(111, projection='3d') #project variable holds the parameters for creating a 3D plot.


plt.scatter(Considered_Values[Predicting_kmeans == 0, 0], Considered_Values[Predicting_kmeans == 0, 1], s = 70, c = 'red', label = 'Cluster 1') #plotting the first cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 1, 0], Considered_Values[Predicting_kmeans == 1, 1], s = 70, c = 'blue', label = 'Cluster 2') #plotting the second cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 2, 0], Considered_Values[Predicting_kmeans == 2, 1], s = 70, c = 'green', label = 'Cluster 3') #plotting the third cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 3, 0], Considered_Values[Predicting_kmeans == 3, 1], s = 70, c = 'cyan', label = 'Cluster 4') #plotting the forth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 4, 0], Considered_Values[Predicting_kmeans == 4, 1], s = 70, c = 'magenta', label = 'Cluster 5') #plotting the fifth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 5, 0], Considered_Values[Predicting_kmeans == 5, 1], s = 70, c = 'orange', label = 'Cluster 6') #plotting the sixth cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
# plt.scatter(Considered_Values[Predicting_kmeans == 6, 0], Considered_Values[Predicting_kmeans == 6, 1], s = 70, c = 'brown', label = 'Cluster 7') #plotting the seventh cluster where color is given, the labelling of the cluster class is defined and other key values are passed that are essential for generating the clusters using the Kmeans clustering algorithm. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.

plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 80, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('K-Means Clustering General 3D ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(n): #defining a new function add that takes n
  return n+1 #returning n+1
Predicting_kmeans=np.array(list(map(add,Predicting_kmeans))) #mapping the values of the allocated clustering class.

# The main reason for doing the above code is because the classes given in the dataset are befinning from 1 but by default the classes that are generated for our training kmeans algorithm
# is beginning from 0 and because of this the comparision cannot be achieved for the actual classed and the allocated classes.
#Thus, to have similiar understanding of the actual classed and allocated classed, either the values of the class of actual classes should be decreased so that it begins from one
#or else the value of the allocated classes should be incremented by 1 (beginning form 1) so that there is a clear understanding.
#Thus, by above function, the value of the allocated class begins from 1.

data_refined8['Clustering Analysis'] = Predicting_kmeans.tolist() #now after the classes are incremently genetaed for the allocated class, we add the class named as Clustering Analysis( which holds the class values for allocated class) to our main dataframe.
data_refined8 #printing the dataframe with new added class as well as the actual class.

import plotly.express as px #importing plotly.express for better visual presentation.

#here we are genetaing two plots at the same time. The first plot is for the actual class, whereas the second plot is for allocated class.

#plotting the clusters based on actual class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined8, x='X1', y='X2', z='X3', color='Class', title= "Data Points According to Original Class") #passing the proper values for the actual class.
fig.show() #displaying the figure 1

#plotting the clusters based on allocated class.
fig = plt.figure(figsize = (15,15)) #defining the diaplay size
fig=px.scatter_3d(data_refined8, x='X1', y='X2', z='X3', color='Clustering Analysis', title= "Data Points According to Class Allocated by Clustering Algorithm") #passing the proper values for the allocated class.
fig.show() #displaying the figure 2

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
result=accuracy_score(data_refined8['Class'], data_refined8['Clustering Analysis']) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", result*100,"%") #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined8['Class'],data_refined8['Clustering Analysis'])) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined8['Class'],data_refined8['Clustering Analysis'])) #printing classification report based on actual class and allocated class

"""## **Hierarchical Clustering for Data 8**"""

#The second clustering technique performed is the Hierarchical Clustering Algorithm.
from sklearn.cluster import AgglomerativeClustering #importing essenatial libraries.
hclus=AgglomerativeClustering(n_clusters=1, affinity='euclidean', linkage='ward') #taking a new variable hclus which represents Hierarchical Clustering and passing total number of clusters and other features in AgglomerativeClustering
y_hclus=hclus.fit_predict(Considered_Values) #assigning the prediction of Considered_Values to y_hclus

#The next part is the visualization part. Herw two plots are drawn, where one shows the clustering by Hierarchical algorithm, whereas the other plot shows the Dendrogram  Plot

import matplotlib.pyplot as plt #importing the required and essential libraries that are required for the plotting.
import scipy.cluster.hierarchy as shc

dendro = shc.dendrogram(shc.linkage(Considered_Values, method="ward", metric="euclidean")) #taking a new variable dendro and assigning the dendrogram plot foundation with by passing essential arguments required for plotting of the dendrogram.
plt.title("Dendrogram  Plot")  #naming the title
plt.ylabel("Distances")   #defining the x-axis
plt.xlabel("Classes")  #defining the y-axis
plt.show()  #displaying the figure 1


fig = plt.figure(figsize = (5,5)) #defining the size for figure 2
#project = fig.add_subplot(111, projection='3d') #if plot is 3D

plt.scatter(Considered_Values[y_hclus==0,0], Considered_Values[y_hclus==0,1], s=100, c='cyan', label = 'Cluster 1') #plotting the cluster 1 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==1,0], Considered_Values[y_hclus==1,1], s=100, c='blue', label = 'Cluster 2') #plotting the cluster 2 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==2,0], Considered_Values[y_hclus==2,1], s=100, c='red', label = 'Cluster 3') #plotting the cluster 3 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==3,0], Considered_Values[y_hclus==3,1], s=100, c='green',label = 'Cluster 4') #plotting the cluster 4 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==4,0], Considered_Values[y_hclus==4,1], s=100, c='yellow',label = 'Cluster 5') #plotting the cluster 5 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==5,0], Considered_Values[y_hclus==5,1], s=100, c='purple', label = 'Cluster 6') #plotting the cluster 6 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.
# plt.scatter(Considered_Values[y_hclus==6,0], Considered_Values[y_hclus==6,1], s=100, c='brown', label = 'Cluster 7') #plotting the cluster 7 on basis of the Considered_Values and y_hclus with s and c parameters. S denotes the size of the cluster, whereas c defines the color for the cluster group.

#plt.scatter(Understanding_kmeans.cluster_centers_[:,0], Understanding_kmeans.cluster_centers_[:, 1],s = 30, c = 'black', label = 'Centroids') #Here, the centers of the clusters are defined by using the values of variable Understanding_kmeans and finding the centers of clusters using .cluster_centers_. S denotes the size of cluster visualizations and c denotes the color for specific cluster group.
plt.title('Hierarchical Clustering Algorithm ') #plotting the title of the figure
plt.xlabel('X-axis') #plotting for x-asis title
plt.ylabel('Y-axis') #plotting for y-asis title
plt.legend()
plt.show() #displaying the figure.

def add(m): #defining a new function add that takes m
  return m+1 #returning m+1
y_hclus=np.array(list(map(add,y_hclus))) #mapping the values of the allocated clustering class.

from sklearn.metrics import accuracy_score #importing accuracy_score from the library sklearn.metrics
hsresult=accuracy_score(data_refined8['Class'], y_hclus) #comparing actual class and allocated class results and assigning it to a new variable result which helps to compare both the classes- the actual one and the allocated one.
# accuracy_score
print("Accuracy is", hsresult*100, '%') #printing the accuracy

from sklearn.metrics import confusion_matrix, classification_report #importing confusion_matrix, classification_report from the library sklearn.metrics
print("ConfusionMatrix",confusion_matrix(data_refined8['Class'],y_hclus)) #printing confusion matrix based on actual class and allocated class
print(classification_report(data_refined8['Class'],y_hclus)) #printing classification report based on actual class and allocated class

"""---



---

**KMeans Vs Heirarchical Summary:** In some cases (Datasets), KMeans Clustering Algorithm works better, whereas in some cases, Heirarchical Clustering Algorithm outperforms the KMeans Clustering Algorithm. However, for Dataset 8, both the clustering methods produce similiar results and accuracy, and this is because of having same number of classes in the actual data for clusters and allocated cluster generated in case of dataset 8.

# **Task- 2**
"""

#importing essential libraries that are required for the execution of the task-2

import pandas as pd #importing pandas
import matplotlib.pyplot as plt #importing matplotlib
import seaborn as sns #importing seaborn
from sklearn import metrics #importing metrics from sklearn
from sklearn.cluster import KMeans #importing Kmeans
from sklearn.preprocessing import StandardScaler #importing StandarScaler
import scipy.cluster.hierarchy as hcluster #importing scipy.cluster
from sklearn.cluster import AgglomerativeClustering #importing AgglomerativeClustering

from google.colab import files #uploading files from the local system
uploaded= files.upload() #shows the uploaded data

df = pd.read_csv('World Indicators.csv') #reading the dataset- World Indicators.csv and assigning it to a new variable df
df #printing df

per = pd.DataFrame((df.isnull().sum())/len(df))  #checking the percentage of null-values
per.columns = ['Percentage of Null Values']
per.style.format({'Percentage of Null Values': "{:%}"})

df.drop(["Energy Usage", "Lending Interest"], axis = 1, inplace=True) #dropping the columns with most null values that do not have any greater significance in view of the entire dataset.
df.head() #printing df after removing the two above columns.

df.info() #printing the info of the dataframe to check the data types

cols=['Business Tax Rate','GDP','Health Exp/Capita'] #selecting the columns that have special characters
df[cols] = df[cols].replace({'\$': '', '%': '',',':''}, regex=True) #replacing the special characters with ' ' from columns- 'Business Tax Rate','GDP','Health Exp/Capita' because we cannot do the computation with the special characters.
df[cols]=df[cols].astype(float) #changing the dataype of above selected columns from object to float, for easier operation and the region and country columns are left as objects.
df #printing the df after replacing the special characters and changing the datatype.

data = df.dropna() #removing rows that contains null values by using dropna function and assigning it to a new variable defined as data.
data #printing the data after removing the rows containing the null values.

data.info() #to view the changes of the dataframe

from sklearn.preprocessing import MinMaxScaler #importing MinMaxScaler from sklearn.preprocessing
scaler = MinMaxScaler() # the main aim of using MinMaxScaler is to scale the min and the max values in form of 0 and 1. By this way, the computation can be achieved because it is very important to have values in a same range to perform clustering.
#For Example, the column Life Expectancy Female has values like 72 , 53, 80, etc but the Mobile Phone Usage has values like 0.1, 0.7, etc, so in this case it can be difficult, next to impossible, to forming the values together. Thus, to make all values in form of 0 and 1 (min and max), scaling is required.
x_minmax = scaler.fit_transform(data.iloc[:,0:16]) #transforming the data till column 16. Columns 17 and 18 are not included because they are objects.

from os import access #importing access from os
# copying the data
scaled_df = data.copy() #copying the data and assigning it to a new variable scaled_df
ac = scaled_df.columns #reading the columns of data and storing it in a new variable ac
ac = ac[0:-2] #location access

# apply normalization techniques
for column in ac:
  scaled_df[column] = scaled_df[column] / scaled_df[column].abs().max() #scaling each value of columns by the absoulte values respective

scaled_df #printing scaled_df

plt.figure(figsize=(16,8))
cor = data.corr() #Calculate the correlation of the above variables to summarize and understand the correleation between the values
sns.heatmap(cor, annot = True) #Plot the correlation as heat map

dd = data.iloc[:,0:16] #reading the columns of data till 16th and assigning it to a new variable dd
res = [] #taking a new empty list res
idv = [] #taking a new empty list idv
for i in range(2,10): #for condition for i in range(2,10)
    kmeans_vals = KMeans(n_clusters=i) #running Kmeans function which stores n_clusters=i and the entire function is assigned to a new variable kmeans_vals
    kmeans_vals.fit(scaled_df.iloc[:,0:16]) #fitting the data kmeans_vals with view of scaled_df
    res.append(metrics.silhouette_score(dd,kmeans_vals.labels_)) #appending silhouette_score based on dd and kmeans_vals.labels_ with res
    idv.append(i) #number of clusters is appended with idv.

lt = list(zip(idv,res)) #taking a new variable lt and assigning list(zip(idv,res)) to it.
df2 = pd.DataFrame(lt,columns=['K Value','Value']) #taking a new dataframe df2 which holds columns 'K Value','Value from lt
print(df2[df2.Value==df2.Value.max()]) #printing df2 with other values.

# Silhouette Score for K means
# Import ElbowVisualizer
from yellowbrick.cluster import KElbowVisualizer #importing KElbowVisualizer from yellowbrick.cluster
model = KMeans() #assigning KMeans() to a new variable model
# k is range of number of clusters.
visualizer = KElbowVisualizer(model, k=(2,10),metric='silhouette', timings= True) #KElbowVisualizer being called with model, k passed and timings being true and the entire function is assigned to a variable visualizer
visualizer.fit(scaled_df.iloc[:,0:16])        # Fit the data to the visualizer
visualizer.show()        # Finalize and render the figure

"""## **1. Use K-means and hierarchical clustering methods to group similar countries together**"""

#chosing no. of clusters as 2 and refitting kmeans model
kmeans = KMeans(n_clusters = 2,init= 'k-means++', random_state = 111) #Calling Kmeans with n_clusters=2 and assigning it to kmeans
kmeans.fit(scaled_df.iloc[:,0:16]) #fitting kmeans

#count number of records in every cluster
pd.Series(kmeans.labels_).value_counts()

preds = kmeans.labels_ #assigning kmeans.labels_ to a new variable preds
kmeans_df = pd.DataFrame(scaled_df) #taking a new dataframe kmeans_df on basis of scaled_df
kmeans_df['KMeans_Clusters'] = preds #adding a kmeans_df['KMeans_Clusters'] to preds

from scipy.cluster.hierarchy import dendrogram, linkage #importing dendrogram and linkage from scipy.cluster.hierarchy
#dendrogram is used for hierarchical clustering
fig = plt.figure(figsize=(12,7)) #defining the size of the figure
ax = plt.axes() #initializing the axes
linkage_data = linkage(scaled_df.iloc[:,0:16],method='complete',metric='euclidean') #passing scaled_df.iloc[:,0:16], method and matric for linkage and assigning it to a new variable linkage_data
dendrogram(linkage_data) #calling dendrogram and passing linkage_data
ax.set_title(f'Dendrogram') #setting the title
plt.show() #displaying the dendrogram

# Getting labels from Agglomearative Hierarchical clustering
hcluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  #AgglomerativeClustering having n_clusters=2, affinity= euclidean and linkage= ward, and assigning it to a new variable hcluster
#the role of affinity is to measure the similarity of the two clusters
hcluster.fit_predict(scaled_df.iloc[:,0:16]) #fitting scaled_df.iloc[:,0:16 with hcluster
hcluster_label = hcluster.labels_ #assigning hcluster.labels_ to hcluster_label

hcluster_df = pd.DataFrame(scaled_df) #taking a new dataframe hcluster_df based on scaled_df
#adding hcluster labels in hcluster_df
hcluster_df['hcluster'] = hcluster_label
hcluster_df.head()

new = scaled_df #taking new variable "new" and assigning scaled_df
fc = new.pop('Region') #poping Region
new.insert(0, 'Region', fc) #inserting region in new at 1st index

#now after above, the first column is region. Now we will be doing same procedure for adding country. After this, country will be at the very first place.
sc = new.pop('Country')  #poping country
new.insert(0, 'Country', sc) #inserting country column to new. Now country will be the first column.
new #displaying new

"""## **2. Use Internal validation metrics to report the cluster quality**"""

#In the next segment, we are using internal validation metrics to report cluster quality

from sklearn import metrics #importing metrics
from sklearn.metrics import pairwise_distances #importing pairwise_distances
from sklearn import datasets #importing datasets
import sklearn #importing sklearn

## for kmeans
sklearn.metrics.calinski_harabasz_score(new.iloc[:,2:20], new['KMeans_Clusters']) # calculating sklearn.metrics.calinski_harabasz_score using the data new. The location of column starts from 2 because 0th (first) column is country and the next column is region, which are not required at the moment for finding the score.

metrics.silhouette_score(new.iloc[:,2:20], new['KMeans_Clusters'], metric='euclidean') #calculating silhouette_score of the new data. Location beginning from 2nd olumnn (not considering 0 and 1)

## for hierarchial Clustering
sklearn.metrics.calinski_harabasz_score(new.iloc[:,2:20], new['hcluster']) # calculating calinski_harabasz_score for new data.

metrics.silhouette_score(new.iloc[:,2:20], new['hcluster'], metric='euclidean') #silhouette_score

!pip install validclust

from validclust import dunn #importing dunn from validclust daatframe
dunnn = pairwise_distances(new.iloc[:,2:20]) #isolating the columns omiiting out the 'Region' and 'Country' columns
dunnn_km = dunn(dunnn,new['KMeans_Clusters']) #finding the Dunn index of Kmeans Clustering by the column 'Kmeans_Clusters'
dunnn_hc = dunn(dunnn,new['hcluster']) #finding the Dunn index of Hierarchial Clustering by the column 'hcluster'
print('The Dunn Index for Kmeans is',dunnn_km) #printing the Dunn Index for both of kmeans and hierarchial clustering respectively
print('The Dunn Index for Hierarchial Clustering is',dunnn_hc)

"""## **3. Report the best clustering solution. Give a detailed list of all the groups and the countries included within the groups**

***->The best clustering method us Kmeans as the values of CH, Silhouette and dunn index are significantly higher which means a better seperation between datapoints***
"""

kmeans_group = new.groupby('KMeans_Clusters')['Country'].agg(list) #initializing kmeans_group which holds the list of countries that comes under a specific cluster- 0,1,2. Aggregate function is used to add all the countries falling under a cluster as a list.
kmeans_group =  pd.DataFrame (kmeans_group, columns = ['Country']) #adding the countries to the dataframe
kmeans_group #displaying kmeans_group

hierarchical_group = new.groupby('hcluster')['Country'].agg(list) #initializing hierarchical_group which holds the list of countries that comes under a specific cluster- 0,1. Aggregate function is used to add all the countries falling under a cluster as a list.
hierarchical_group  =  pd.DataFrame (hierarchical_group, columns = ['Country']) #adding the countries to the dataframe
hierarchical_group #displaying hierarchical_group

"""## **4. Generate three different scatter plots of your choice and color the data points according to the group.Example: “Life expectancyvs GDP”, “Infant Mortality vs GDP”, etc.**"""

#In this segment, we generate three different scatter plots according to the group.
#Scatter Plot 1: GDP vs Infant Mortality Rate over the two clustering methods
sns.scatterplot(new['GDP'],new['Infant Mortality Rate'],hue='KMeans_Clusters',data=new)  #scattering between GDP and Infant Mortality Rate using the data - new.
plt.title("Child Mortality vs GDP for kmeans clustered data", fontsize=15) #title for the plot
plt.xlabel("Child Mortality", fontsize=12) #labeling x-axis with also defining the font size.
plt.ylabel("GDP", fontsize=12) #labeling y-axis with also defining the font size.
plt.show() #displaying the scatter plot of GDP vs Infant Mortality Rate

sns.scatterplot(new['GDP'],new['Infant Mortality Rate'],hue='hcluster',data=new)  #scattering between GDP and Infant Mortality Rate using the data - new.
plt.title("Child Mortality vs GDP for hierarchial clustered data", fontsize=15) #title for the plot
plt.xlabel("Child Mortality", fontsize=12) #labeling x-axis with also defining the font size.
plt.ylabel("GDP", fontsize=12) #labeling y-axis with also defining the font size.
plt.show() #displaying the scatter plot of GDP vs Infant Mortality Rate

#Scatter Plot 2: Life Expectancy Male vs Population of 65+ Life for respective clustering
sns.scatterplot(new['Life Expectancy Male'],new['Population 65+'],hue='KMeans_Clusters',data=new) #scattering Life Expectancy Male vs Population of 65+ Life  using the data - new.
plt.title("Life Expectancy Male vs Population of 65+ for kmeans clustering", fontsize=15) #title for the plot
plt.xlabel("Life Expectancy Male", fontsize=12) #labeling x-axis with also defining the font size.
plt.ylabel("Population of 65+", fontsize=12) #labeling y-axis with also defining the font size.
plt.show() #displaying the scatter plot of Life Expectancy Male vs Population of 65+ Life

sns.scatterplot(new['Life Expectancy Male'],new['Population 65+'],hue='hcluster',data=new) #scattering Life Expectancy Male vs Population of 65+ Life  using the data - new.
plt.title("Life Expectancy Male vs Population of 65+ for hierarchial clustering", fontsize=15) #title for the plot
plt.xlabel("Life Expectancy Male", fontsize=12) #labeling x-axis with also defining the font size.
plt.ylabel("Population of 65+", fontsize=12) #labeling y-axis with also defining the font size.
plt.show() #displaying the scatter plot of Life Expectancy Male vs Population of 65+ Life

#Scatter Plot 3: Mobile Phone Usage vs Internet Usage for respective clustering
sns.scatterplot(new['Mobile Phone Usage'],new['Internet Usage'],hue='KMeans_Clusters',data=new) #scattering Mobile Phone Usage vs Internet Usage using the data - new.
plt.title("Mobile Phone Usage vs Internet Usage for kmeans clustering", fontsize=15) #title for the plot
plt.xlabel("Mobile Phone Usage", fontsize=12) #labeling x-axis with also defining the font size.
plt.ylabel("Internet Usage", fontsize=12) #labeling y-axis with also defining the font size.
plt.show() #displaying the scatter plot of Mobile Phone Usage vs Internet Usage

sns.scatterplot(new['Mobile Phone Usage'],new['Internet Usage'],hue='hcluster',data=new) #scattering Mobile Phone Usage vs Internet Usage using the data - new.
plt.title("Mobile Phone Usage vs Internet Usage for hierarchial clustering", fontsize=15) #title for the plot
plt.xlabel("Mobile Phone Usage", fontsize=12) #labeling x-axis with also defining the font size.
plt.ylabel("Internet Usage", fontsize=12) #labeling y-axis with also defining the font size.
plt.show() #displaying the scatter plot of Mobile Phone Usage vs Internet Usage

"""

---



---

"""